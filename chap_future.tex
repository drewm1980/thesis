\section{Introduction}

The previous two chapters covered work that I have already performed towards my
Ph.D. in the area of face recognition.  In this chapter I propose the main
components of plan to turn our face recognition system into a complete package
that is both accurate and scalable.  I haven already demonstrated that
state-of-the-art face recognition performance can be achieved through careful
control of the training image acquisition process and the use of an algorithm
that effectively models illumination, pose, and occlusion.  Unfortunately,
while our CPU implementation already performs several times as fast as our
MATLAB implementation, it currently still takes over a minute to process a
single test image.  This is clearly far too slow to be useful for an access
control system. Since this is the biggest bottleneck for adoption of the
proposed face recognition technology, solving this problem will be a major
component of the remaining work for my proposed Ph.D. thesis.  Another major
component of my research will be a systematic study of the choice of training
illuminations.  Having already implemented a novel projector based training
image acquisition system, I have the unique chance to automate the optimization
of the set of training illuminations.  Another major step will be to combine
the Markov Random Field occlusion handling with our iterative alignment
algorithm.  Finally, there are some improvements that could be made to the
training acquisition hardware itself to improve training image acquisition.  

\section{Optimizing the Speed of the Recognition Algorithm}

Before any progress can be made on improving the speed of the algorithm, we
must first understand why it currently runs so slowly. The main reason is that
the sparsity based alignment and recognition algorithms detailed in the
previous chapters use entire images as features.  Many face recognition
algorithms either start with much smaller features to begin with (i.e. image
patches, SIFT features), or project the images down to a much lower dimension
as a first step.  Not only are we using relatively large training images in
their natural basis, we are performing optimizations on them repeatedly in a
loop, instead of just once, as is the case with Eigenfaces.   These
optimization routines involve operations on many of these images at a time (for
example, the recognition step currently uses 38 training images).  This is a
very expensive algorithm in terms of the amount of data that must be handled
simultaneously, and would have been considered very impractical to implement
until rather recent improvements in computing hardware.  While we have an
enormous amount of data that must be operated on, the simplicity of our choice
of features has resulted in an algorithm that is extremely data parallel.
Table \ref{tab:breakdown} shows a breakdown of the execution time for the
different parts of the algorithm.  The first thing to note is that the
algorithm's execution time will have to be improved by a factor of about 100 in
order to be useful for access control.  It is unreasonable to expect a user to
wait for much more than a second while the system decided whether or not to
unlock the door.  Due to the emergence of readily available massively parallel
processors in the form of programmable GPU's, this factor of speed improvement
is actually more reasonable than it sounds.  Similar speedups have actually
been achieved for some other data parallel tasks when ported from the CPU to
the CUDA architecture.  The next few subsections will outline some strategies
for optimizing the individual steps.
\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
\hline
Operation & Execution Time (seconds)\\
\hline
Loading the training database & 74\\
\hline
Per-user alignment & 130\\
\hline
Resampling the training images & 18\\
\hline
Final recognition step &137\\
\hline
\end{tabular}\vspace{2mm}
\caption{Execution timing for a database of 100 training users with 30 xga grayscale training images per user} \label{tab:breakdown}
\end{table}

\subsection{Loading the training database}  The training database can be
potentially quite large; for a database of 100 users, the full resolution
training images take up 3 GB of data.  While his can be reduced significantly
for the per-user alignment step (we only need the smaller re-sampled training
images), we will need to load the full resolution images for the users that
make it to the final recognition step, and this access must happen with very
low latency.  Clever management of the loading of training images into memory
will clearly be necessary.

One strategy for reducing the latency of the image loading is to use a
technology called memory mapping.  This creates a mapping between the data in a
file in the system (most of which will be resident on the hard drive) and the
address space of the program.  The operating system's virtual memory system
then handles the loading of data from disk into RAM as needed when the program
access the corresponding addresses in its memory space.  This technique has the
potential to kill several birds with one stone:
\begin{itemize}
\item Since there is one virtual memory system for all processes in the os,
portions of the database that are used by multiple processes can share the data
that has been loaded into RAM.  This can significantly reduce the memory
footprint when multiple instances of the recognition system are running
simultaneously.
\item It is much easier to load just the portion of each image file that is
needed.  Resampling the training images will only require a subset of the
pixels from the high resolution image.  On x86 systems the memory page size is
2KB.  For a grayscale xga image laid out linearly one disk this corresponds to
two rows of the image.  In this case only the rows that overlap the user's face
will get loaded from disk into memory.  This can be improved further by laying
out the data so that each memory corresponds to a tile of about 45 x 45 pixels.
Then only the tiles that overlap the users' face will have to get loaded into
memory.
\item There is a mechanism to hint to the operating system what data will be
needed in advance so that is can be pre-loaded.  
\end{itemize} 
Another strategy for reducing the latency of the image loading is to try to
predict which images will need to be loaded in advance of when they are used.
In particular, we have some information about which users will likely make the
cut for the recognition step while per-user alignment is still being performed.
Any training user that is not in the top $S$ (currently set to 10) users
aligned so far certainly need not be loaded into memory.  

\subsection{Per-user alignment}  
Since this operation must be performed once per training user, the cost of this
operation grows proportionally with the size of the training database.  For
each user, the computation for this step involves repeatedly solving an l1
optimization problem and resampling filtered versions of the test image.  

One strategy for reducing the amount of time spent resampling(warping) the
training images is to make use of the fact that we are resampling $N$ (= 38
training images currently) simultaneously with the same transformation.  We can
use this fact in two ways: first, we only have to compute the mapping of the
pixel locations once, and second, we can tile the data for all 38 images
together to further reduce time spent accessing memory.   

\subsection{Resampling the training images}  
Again, clever tiling and pre-fetching will be able to greatly reduce the time spent in this task.

\subsection{Final recognition step}
This bulk of the time spent in the final recognition step is in a large l1 optimization.  The matrices involved are bigger, but the l1 optimization problem only has to be solved once.  

The two l1 optimization problems are solved by re-casting them as linear
programming problems.  The resulting linear programming problems can be solved
efficiently using interior point methods.  The most expensive operation in the
interior point method for these problem sizes is the computation of the step
direction.  This requires a multiplication of several large matrices to compute
the matrices for a much smaller linear system of equations.  This smaller
linear system is then solved by minimizing the $\ell^2$ norm of the error.
Since all of these operations are taking place on very large arrays, these
optimizations have the potential to map well to massively programming
architectures, such as GPU's.  The massively parallel (capable of executing
hundreds of threads concurrently) architecture that is currently the most
promising is Nvidia's CUDA architecture, and development will center on this
development system for the foreseeable future.  Performance gains of up to 100x
over CPU implementations have been reported for algorithms that map well to the
GPU.

An example of an operation in our algorithm that is very low hanging fruit for
GPU implementation is the multiplication of $A^T * D * A$ where $A$ is
extremely tall (5120 x 38) and $D$ is diagonal.  While a general GPU
implementation of matrix multiplication is provided with the GPU, there are
likely significant performance improvements to be gained from a custom
multiplication routine.  Data can be shared between the left and right copies
of A.  To conserve cache space $A$ can be stored as the original 8 bit data
type that came from the image, along with a floating point scale factor for
each column.  The fact that $D$ is diagonal also greatly reduces the amount of
computation that need be done.  Since $A$ is extremely tall, the edge case (in
computation and in memory loading) for  the long edge is much more important
that for the short edge.  Furthermore, the prior knowledge that $D$ gets
updated about 10 times as frequently as $A$ how best to handle memory
transfers.

While the efficient implementation of the sparse representation algorithms we
used has a limited interest from a theoretical standpoint, it is absolutely
critical for enabling the use of these techniques for vision applications.
Sparse representation may one day be as important a building block for computer
vision as the singular value decomposition is currently, but this cannot happen
without efficient implementations that are tuned both for the application
domain and the available hardware.  

\section{Optimizing the training illuminations} 
To date, I have performed two experiments have been performed to guide the
choice of training illuminations, the results of which were shown in Chapter
\ref{chap:cvpr}.  One used rectangular partitions of the domain of the
illumination space with different granularities.  This gave a rough idea of the
number of training illuminations that would be necessary.  The second
experiment used a partitions of the illumination domain that were rectangular
in a polar coordinate system.  This gave a rough idea of the range of angles
the training images should cover.  Our large training database was acquired by
combining these two pieces of information.  While a study of the choice of
training illuminations at this granularity is already unprecedented for face
recognition, \footnote{ Several studies, including \cite{Basri2003-PAMI,
LeeK2005-PAMI} have used images rendered from a 3D face model to optimize their
choice of illumination.  Similar illumination studies have also been performed
on other objects for graphics applications.} only a very small subset of the
possible illuminations was explored compared to what the projectors are capable
of.  For instance, how many of the frontal images are necessary?  Should some
directions receive a higher density of illuminations than others? 

\subsection{Optimize illuminations within the set of training images we already have}
One option we have for optimizing our training illuminations is to use the
training and testing databases we have already gathered, and to search for a
lower dimensional subspace of illuminations that still has a good recognition
rate.  The two main advantages of this strategy are that the recognition rate
can be computed directly and used as a measure of the quality of the training
images, and that the sizable database we have already gathered can be used.
The main disadvantage is that we will most likely have to trade some accuracy
for speed, since we can only restrict the number of measurements the algorithm
uses. \footnote{It could be possible for a subset of the illuminations is more
discriminating than the full set; however this would be a very surprising
result indeed!}

\subsection{Optimize illuminations with the illumination system in the loop}
A second option is to perform an optimization of the training image space with
the image acquisition system in the loop.  The main advantage is that we can
include any training images we want to in the optimizaztion.  The main
disadvantage is that we cannot compute a recognition rate and instead have to
resort to using representation error as a measure of the quality of the
training images.  Why can't we just capture and store a the images generated by
a complete basis for the space of illuminations, and then optimize over
weighted combinations of them?  If a single projector pixel is illuminated at a
time, it would take over 4 TB of data to store the remaining images, and take
28 hours to capture; this wouldn't be convenient, but it could be managed.
Unfortunately, there is a problem with this idea:  in every image you take, the
actual signal would fall below the noise floor of the camera.  For this reason
it is critical to capture images with a significant portion of the projector
pixels illuminated at a time.  Keeping the real world in the loop solves this
problem without resorting to arbitrarily enforcing a minimum block size.  Due
to the extremely long time that the subject will have to remain motionless
while the optimization runs, a movie-grade dummy head would be a good candidate
for the training subject.  

This still leaves several interesting questions.  Even with an automated search, we are going to have to make some assumptions to reduce the search space.  Should we allow for pixels to be partially turned on, or should they be binary?  Should we require illuminations to have pixels that are all adjacent?  What metric should we use to measure the quality of the training images?  Ideally we'd be using a large database of real subjects, but that is not an option if we want to search over the full set of illuminations the projectors can generate.  How many training images should we allow?  How do we quantify the tradeoff between speed and accuracy?

There are few other things that make this experiment appealing.  Since the dummy head is inanimate, we will completely eliminate the influence of pose variation on the experiment, and all of our training images will already be perfectly aligned; the cameras can be manually positioned such that the frontal and rear illuminations match up perfectly. \footnote{Since we want to capture training illuminations from both sides, the dummy head can be mounted on a stepper-motor driven pivot for the purpose of rotating the dummy head by 180 degrees.  }

\subsection{Leverage color information to improve occlusion robustness}  Color information is unused for the current system.  This was a simplifying assumption that made implementation of the algorithm significantly easier and faster.  There are several different ways in which the algorithms presented in this thesis could be extended to handle color information.  One of the main reasons that color is not as important in face recognition as it is in some other vision applications is that the pixels in the image of a given person's face generally vary primarily in intensity.  Furthermore, the variation of skin tone from person to person usually varies less than the color variation resulting from the color of the illumination.  For these reasons, color may be especially useful for the improvement of occlusion handling, since occluding objects are likely to vary in color more than human faces do.  


\section{Combining MRF recognition with alignment} Chapter \ref{chap:iccv} demonstrated that it is possible to improve recognition performance by leveraging the information that occlusions are typically spatially coherent.  However, all of the experiments were performed on images that had already been aligned.  Combining this technique with alignment is an important topic for further research, and my collaboration with Zihan Zhou will continue on this topic.  The first attempt at integrating the MRF extension with alignment will be a straightforward combination of the two ideas.  Alignment is achieved by including a linearization of the warped test image with respect to the transformation parameters in the L1 optimization.  MRF simply restricts the subset of pixels on which the L1 optimization is performed.  Therefore, these two ideas are neatly orthogonal from an implementation standpoint.  The challenging part will be to understand exactly how the alignment and occlusion rejection interact in the early iterations of the algorithm, where the estimates of both the occlusion and the alignment have not yet converged significantly.


\section{Improving the acquisition system hardware}
The current design has proven to be very satisfactory in a research environment where the subjects are very cooperative and make an effort to hold very still while the images are being taken.  Unfortunately, not all users of this technology will be so careful.  The best way to improve the quality of the training images is to increase the speed at which they are acquired.  The primary motive for this is to reduce the amount of movement of the user's head between consecutive training images.  There are several possible ways in which this goal can be pursued:
%{\bf Refinements to the current design}
\begin{itemize}
\item {\em Increase the image acquisition rate by upgrading the cameras.}  Some newer IEEE1394 cameras contain features that could facilitate this.  One is the ability to trigger off of an external signal while still interleaving image exposure with transfer of the previous frame over the bus.  With our current cameras transfer and integration is serialized.  
\item {\em Implementing some type automatic exposure control}  Some illuminations cause more light to fall on the subject's face than others.  Therefore, the optimal exposure time depends on which illumination is being displayed.  Automatic compensation for this could reduce the average exposure time without hurting the SNR.
\item {\em Reduce synchronization delays by modifying the projectors}  There is a clock signal that drives the switching of the Digital Micromirror Device in each projector.  Synchronizing the camera exposure with a whole number of projector frames would greatly increase acquisition time; currently we have to be very conservative with the amount of time a given illumination is displayed to prevent problems synchronizing with the camera.
\item {\em Reduce exposure time by modifying the projectors}  The only way to decrease the exposure time without hurting signal to noise ratio is to increase the intensity of the lighting.  Most DLP projectors achieve color by spinning a wheel with either a colored mirror or a transmissive color filters in front of the light source.  Since color is not needed for  the training image acquisition, it would be desirable to remove the color wheel, effectively turning the projector into a black and white projector with a higher maximum intensity.
\end{itemize}
Not all of the above strategies may turn out to be feasible; in particular, the last two ideas would require some degree of reverse engineering of proprietary hardware inside the projector.  While the modifications are conceptually very simple, there may be unforeseen difficulties depending on the design of the projector.  In the worst case, the manufacturer may have deliberately implemented features to prevent tampering.

% TODO Mention scheneker illumination patterns.

%{\bf Radical departures from the current design}
%The training image acquisition system detailed in Chapter \ref{chap:cvpr} requires several reasonably high quality DLP projectors and cameras for its operation.  While this is an investment that a large institution using face recognition  for access control might be willing to make, it pretty much rules outs application in less mission critical applications. There are several substitute techniques that may be worth pursuing:
%\begin{itemize}
%\item {\em Replace the projectors with many smaller light sources}  The main disadvantage of this technique is that it necessitates either building a large structure to hold the light sources or mounting them on the walls and ceiling.  Either way, a network of wires will be required to power and the actuate the lights, which further complicates installation.
%\item {\em Replace the projectors with a moving light source}  The main reason the projectors are not very cost effective in this application is that most of the light they produce is wasted.  This dramatically increases the need for thermal design, reduces the life of the components, and increases the power usage.  There are a variety of possible designs that get around this including the use of a moving light source, moving light-directing mirrors, or an array of focused light sources.
%\item {\em Replace the projectors with ambient lighting}  One face recognition application that has already hit the market is for securing user login on laptops.  Unfortunately, most individual users won't have access to a system that takes many pictures of them under controlled lighting.  One potential solution to this could be to have the user turn on a single light in their room, and then turn around in a circle while holding their (camera equipped) laptop.  The biggest hurdle for this technique will be that the training images will need to be aligned, since the user's head will likely move significantly during use.
%\end{itemize} 

\section{Conclusion}
Accelerating the algorithm to the point where it can be used for access control
will initially be the highest priority.  Almost all parts of the algorithm will
need to be re-implemented to take advantage of hardware specific optimizations
on a new architecture, and this will be a major undertaking.  However this is
effort is necessary not only to demonstrate that the recognition system can be
applied to access control, but also so that the L1 optimization routines we use
can be useful to others in the vision community.  Furthermore a faster
implementation will enable me to perform experiments in the optimization of the
training illuminations at a granularity that has never been attempted before.
It will be very interesting to see how much a more thoroughly optimized set of
training illuminations can improve the recognition rate, and what those
illuminations look like.  If the optimization of the training illuminations
reliably converges to a particular set of illuminations, this work could settle
once and for all how training illuminations for face recognition should be
selected.  The robustness of the recognition algorithm to occlusions can likely
be improved by integrating our new MRF results with our existing iterative
alignment algorithm, and the quality of the training images can be improved
through improved hardware integration.  The overall goal of the project is to
deliver a face recognition system that is easy to implement, is robust to
illumination variation, mild occlusions, and mild pose variations, and is
scalable up to hundreds of users.
