\chapter{}
\label{chap:appendix_illumination}

This appendix provides a more in-depth discussion of the linear illumination
model used in the rest of the thesis.  The illumination model has important
implications for the design of the training acquisition system, as well
as important implications for how the $\ell_1$-minimization problems should
be formulated.
A linear illumination model assumes that the test image, although taken under some
arbitrary illumination, can be linearly interpolated by a finite number of
training illuminations.  This idea is certainly not new; indeed, it has been in
use for face recognition for roughly two decades\cite{Turk1991-CVPR}.  Under
what conditions is this a reasonable assumption to make?  What can we say from
first principles about how the training images should be chosen?  
Section \ref{sec:illumination_derivation} explains the physical justification
for using a linear illumination model, and Section \ref{sec:positivity}
discusses some important details of how the illumination model should be 
applied.

\section{Derivation of the Illumination Model}
\label{sec:illumination_derivation}
%
First, let us assume that the illumination if the subject's face is distant.
This model will be a good approximation as long as distance to the nearest
light source is much larger than the person's face.  If we further assume that
the object is convex so that there is no self-shadowing, the illumination
incident on a surface patch will depend only on its orientation, and not on its
position.  Furthermore, if the object is Lambertian, the intensity of the image
of a given patch of object will not depend on its position either.  Since each
pixel in the image corresponds to some patch on the object, the vector of image
intensities is a linear function of the radiance of the corresponding portions
of the object.  Consider the vector space of all illuminations of the object.
These are just positive functions (or more generally, distributions) defined on
the sphere, where each point on the sphere corresponds to a direction of
incoming light.  Consider the vector space formed by the light exiting the
object.  This too is a positive function defined on the sphere, where now each
point on the sphere corresponds to the normal of a patch on the object (or any
other patch with the same normal).  Under the Lambertian assumption, the
radiance of a patch depends on the cosine of the incident angle of incoming
light, and is integrated over the half-sphere of illumination that the patch
can ``see."

Thus, the surface of the object acts on the sphere in a manner very similar to
a half-cosine approximation to a low-pass filter for a one-dimensional signal.
Thus the energy leaving the object is disproportionately concentrated in the
subspace corresponding to low spatial frequencies.  Since the image is a linear
function of this, the space of images of an object will also tend to fall on
(the positive portion of) a subspace.   In \cite{Basri2003-PAMI}, Basri showed
using spherical harmonic basis functions that nine basis illuminations
(corresponding to the lowest frequency spherical harmonics) result in training
images that do a good job of linearly interpolating all other training images.
It should be noted, however, that these basis illuminations are not strictly
positive, and thus neither are the training images they generate (when rendered
on a computer that can handle negative illumination).\footnote{Note that
while the spherical harmonic basis functions have regions where they are
negative, this does not necessarily preclude this theoretical result from being
put to use in practice.  If the geometry of the training illumination system
were well calibrated, it would be possible to generate approximations of the
positive and (rectified) negative components of the basis illuminations
separately, taking the difference of the resulting two images, and storing it
using a signed datatype.  Combinations of these images may not be positive, but
they may still be good enough to be useful.}

Although a human face is neither perfectly Lambertian nor convex, it has been
observed in various empirical studies that one can often get away using a
similar number of frontal illuminations to interpolate a wide range of new
frontal illuminations that taken under the same laboratory conditions
\cite{Georghiades2001-PAMI}. This is the case for many public face data sets,
including AR, ORL, PIE, and Multi-PIE.  Unfortunately, we have found that in
practice, a training database consisting purely of frontal illuminations is not
sufficient to linearly interpolate images of faces taken under typical indoor
or outdoor conditions (see the experiment conducted in Section
\ref{sec:own-data}). As illustrated by the example in Figure \ref{fig:promo},
an insufficient number of training illuminations can result in recognition
failure.  To ensure our algorithm works in practice, we need to find a set of
training illuminations that are indeed {\em sufficient} to linearly interpolate
a wide variety of practical indoor and outdoor illuminations.

\section{Enforcing Positivity in the Representation Coefficients} 
\label{sec:positivity}
One critical issue in linear illumination models
is whether to enforce non-negativity in the coefficients $\x$,
i.e. whether to model illumination using a cone or a subspace.
Some authors, i.e. \cite{Basri2003-PAMI}, have chosen to allow
negative components in the coefficient vector $\x$ when
representing one image as a sum of other images, and only
enforce that the linear combination $A\x$ be positive
everywhere.  Unfortunately, for non-convex objects such as
faces, enforcing non-negativity of the representation is not
sufficient to guarantee that the resulting image of the object
is physical.  For example, consider a Lambertian planar object
with a cylindrical hole drilled in it that is aligned with the
optical axis of an orthographic camera.  Consider two images:
one under illumination from a distant point source aligned with
the hole, and one off of the hole axis.  Positively weighting
the on-axis image and negatively weighting the on-axis image
can result in an image where the bottom of the hole is more
strongly illuminated than the surrounding plane.  The image is
positive, but it is clearly non-physical.  It is worth
emphasizing that this phenomenon is a result of some points on
an object seeing a restricted subset of the illumination sphere
(a violation of convexity),  and not a result of the
directionality of the light chosen in the thought experiment.
While the number of coefficients that actually end up negative
after optimization is usually very small, we have observed
pathological cases where the shadows next to the user's nose in
the reconstructed image invert in value.

Instead of using a smaller set of training images, or allowing the coefficients
to go negative to try to represent more test illuminations (and risking
over-fitting), we decided to enforce non-negativity of the coefficients.
Non-negative combinations of images are guaranteed to correspond to physically
plausible images.  Because we have a flexible acquisition system, we can
directly generate a set of illuminations that span most of the illumination
cone, without resorting to negative coefficients and risking over-fitting.  In
situations where many training illuminations were captured (and the ambient
light image is subtracted), we typically enforce that the coefficients $\x$ be
non-negative.  This additional restriction of the solution has also been found
to be beneficial when the test image was taken under extremely poor lighting
conditions.


