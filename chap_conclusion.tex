\chapter{Conclusion}
\label{chap:conclusion}

Humans have evolved an amazing capacity to recognize each other with just a
glance at a person's face, from any angle, under almost any lighting condition,
and even if only a portion of the face is visible.  While we take it for
granted, the ability to quickly identify the people around us is critical to
our social interaction and often to our personal safety.  While we are often
able to recognize people we know by their gait, their voice, or even what they
are wearing, face recognition is the fastest and
most reliable way to identify people in our surroundings.  We are comforted by
seeing the faces of people we know and trust, and we are often ill at ease when
we are in the vicinity of someone who is concealing their identity with a mask.  

Face recognition is such a key component of human interaction that automatic
face recognition has the potential to be useful in almost any situation where a
human is interacting with a machine.  Indeed, automatic face recognition has
already become a large industry, even though, despite three decades of
research, the technology is still immature and very few applications have
actually been successful.

While the recognition system developed in this thesis is still far from
matching the robustness of the human recognition system, it demonstrates that
there is a largely ignored class of recognition applications for which face
recognition is tractable.  By gathering a sufficient set of images of each
subject under different illuminations and fixed pose, we are able to robustly
recognize subjects under lighting from any direction.  This parallels how
humans become familiar with the appearance of a face. Even if we only meet
someone briefly, as their head moves, so does the relative direction of the
light illuminating it, and thus we always see their face under varying
illumination.  While the necessity of gathering these gallery images increases
the cost of the system, this is outweighed by the robustness advantages.

Until quite recently, a recognition system of the type presented in this thesis
would have been extremely impractical.  In particular, until the advent of
Digital Micro-mirror Devices (a.k.a. DLP) by Texas Instruments, most projectors
had far lower contrast and lower brightness, and were thus much less suitable
for building a light stage like the described acquisition system.  Furthermore,
without the recent dramatic increases in computational bandwidth achieved by
modern parallel processors, the direct use of raw images as features was
prohibitively expensive; low dimensional feature extraction was a practical
necessity.  Finally, an improved understanding of the mathematical properties
of the $\ell_1$-norm, both as a robust error function, and also as a
sparsity-encouraging regularization term, enabled the formulation of recognition in
terms of convex optimization problems without sacrificing robustness to sparse
error (as with traditional $\ell_2$ norm error based methods).  The recognition
system presented here would have not been possible prior to the emergence of
these three technologies.

\section{Summary of Contributions}
While the primary value of this research is in the overall integrated approach taken by
the recognition system, there are several contributions that stand out, and could be
integrated into other recognition systems.  These contributions
are the acquisition system, the robust iterative alignment formulation,
improved $\ell_1$ minimization algorithms, and optimized parallel
implementations for both CPU and GPU architectures.  Each of these
contributions is briefly reviewed.

The acquisition system quickly captures images of the subject under a set of
illuminations that closely models the wide range of lighting distributions in
the real world.  It was shown empirically that 38 illuminations are sufficient
to accurately model a variety of real-world illumination conditions.  In
particular, experiments demonstrate that rear illuminations are important.  The
illumination generated by the system is distant, and with only two projectors
it covers well over a hemisphere of incident illumination directions; only a
cone of illumination directions from below horizontal is not covered.  With the
addition of a projector that bounces light off of the floor, the system could
even achieve true omnidirectional illumination coverage, though it remains to
be seen if this is worthwhile.  All of the hardware used is available off the
shelf, and the cost of the components is dropping rapidly.\footnote{The cost of
DLP projectors has roughly halved since the construction of the first
prototype.} The cost of the overall system is already comparable to the cost of
the illumination equipment already used in photography studios.  

The robust iterative alignment formulation successfully combines Lucas-Kanade
style iterative alignment with the $\ell_1$ norm as a robust error function,
and with an illumination model formed from the linear span of multiple gallery
images per subject.  It exhibits a region of attraction that is more than
sufficient to work with the output of the de-facto standard Viola-Jones face
detector.  In addition to boosting the performance of a SRC-based recognition
stage, iterative alignment is also shown to boost the performance of a
recognition stage based on local binary patterns.  Experiments on the Multi-PIE
dataset demonstrate that while this improves recognition rate, SRC remains
significantly better at rejecting impostors.  Isolating the cause of this and
designing a recognition stage with the strengths of both is an important topic
of future research.

In addition to the interior-point based $\ell_1$ solvers that were developed
for the first system prototype, solvers based on ALM were developed that
significantly reduce the amount of required computation, and are simpler 
to implement on parallel architectures.  In contrast to the matrix-matrix
operations and data re-ordering that are required by interior point solvers,
the inner loops of the ALM solvers are composed primarily of matrix-vector
multiplications and vector-vector operations.

Even with the improved solvers, optimized implementations running on powerful
parallel hardware are necessary to achieve recognition speeds that are
practical for access control systems (and many other applications by
extension).  While use of vendor-supplied parallel BLAS libraries is sufficient
to achieve efficient parallelization of the SRC based recognition stage, for
the alignment stage it is necessary to take advantage of the additional level
of parallelism in the algorithm that arises from performing alignment
individually for each gallery user.  On the CPU, this was achieved by using a
combination of single-threaded BLAS libraries, manual threading via the OpenMP
API, and auto-vectorization compiler features.  For alignment on the CPU, the
highest performing parallelization is achieved by solving one iterative
alignment problem per CPU core.  In contrast with the CPU, on the GPU, there
are no vendor-supplied libraries that operate at the granularity of a single
core (Streaming Multiprocessor).  The vendor supplied CUBLAS library is
designed to take advantage of the whole GPU to solve a single large problem.
It was determined experimentally that the alignment problems are much too small
to run efficiently on the GPU sequentially. It was therefore necessary to
develop an implementation of the alignment stage from scratch (i.e. no library
dependencies) that solves many alignment problems concurrently.  The optimized
implementations on both architectures are at least an order of magnitude faster
than their sub-optimally parallelized predecessors.  The CPU and GPU
architectures are competitive with each other in speed.  On the CPU the problem
fits in L3 cache and speed is governed by the number of available cores.  On
the GPU the data must be streamed from DRAM, and speed is governed by memory
bandwidth.  Furthermore, new architectures are emerging that combine the best
aspects of CPUs (ample amounts of cache) and GPUs (massive concurrency).
Examples of this are AMD's Fusion Accelerated Processing Unit and Intel's
Knights Corner.  The parallelization lessons learned on the current generation
of hardware will almost certainly apply to the next generation of hardware as
well.

\section{Future Work} Experience with the recognition pipeline suggests that there are
several important directions for future research.  It will be important to
determine exactly why SRC has such a strong ability to reject impostors, so
that this effect can be engineered into other systems.  Furthermore, broadening
the range of applications will require further improvements in robustness to
occlusions, robustness to pose variation, and further optimization of the
gallery images.  These three directions are discussed below.  

Our experience so far indicates that that it is difficult to find an occlusion
model that improves robustness without de-stabilizing iterative alignment or
sacrificing generality.  Unless a suitable occlusion model can be found, it may
be necessary to resort to more expensive brute force techniques. For example,
taking inspiration from {\em Random Sampling and Consensus} (RANSAC), candidate
occlusion regions could be randomly sampled, and a voting scheme could be used
to choose among candidate alignment update directions.  Similarly, multiple
alignment initializations could be tried. 

Research into robustness to pose variation could go in several directions.  One
direction currently being pursued is to use a more general class of
deformations that is better able to represent the image warping induced by pose
variation of the 3D face.  One danger with this technique is that adding more
degrees of freedom increases the danger of over-fitting the data and
introducing convergence problems.  Another direction that could be pursued is
to combine our 2D techniques with 3D gallery data, while still using 2D test
images.  In this case, the Jacobian used in iterative alignment would be
enhanced to include out of plane pose variation.

The training image acquisition system could be improved in several ways.  As
already mentioned, it may be advantageous to to capture below horizontal
illumination directions.  Furthermore, it may be possible to capture images
with a better SNR in a shorter time by using multiplexed illuminations
\cite{schechner2007multiplexing}.  Multiplexed illuminations do have an
important downside. Enforcing positivity of the illumination coefficients 
$\x$ improves robustness in images that are taken with very poor SNR.  For this
to be possible, the multiplexing must be invertible.  This in turn requires
that the camera and projectors must either be carefully calibrated, or the
brightness of the illumination for each image must be measured, increasing the
cost and complexity of the acquisition system.  For data sets such as
Multi-PIE, where the test images do not have gross errors, non-negativity of
$\x$ need not be enforced, and inversion of the multiplexing becomes
unnecessary.  It is likely also possible to find a smaller set of illuminations
that still sufficiently model real-world illuminations, especially if
additional
information about the illumination in the test image is available.  For
example, for access control applications, a subject entering a building is
typically strongly backlit.




%This still leaves several interesting questions.  Even with an automated
%search, we are going to have to make some assumptions to reduce the search
%space.  Should we allow for pixels to be partially turned on, or should they
%be binary?  Should we require illuminations to have pixels that are all
%adjacent?  What metric should we use to measure the quality of the training
%images?  Ideally we'd be using a large database of real subjects, but that is
%not an option if we want to search over the full set of illuminations the
%projectors can generate.  How many training images should we allow?  How do we
%quantify the trade-off between speed and accuracy?

%There are a few other things that make this experiment 
% STEPH: what experiment?
% appealing.  Since the
%dummy head is inanimate, we will completely eliminate the influence of pose
%variation on the experiment, and all of our training images will already be
%perfectly aligned; the cameras can be manually positioned such that the
%frontal and rear illuminations match up perfectly. \footnote{Since we want to
%capture training illuminations from both sides, the dummy head can be mounted
%on a stepper-motor driven pivot for the purpose of rotating the dummy head by
%180 degrees.  }

%\subsection{Leverage color information to improve occlusion robustness}  Color
%information is unused for the current system.  This was a simplifying
%assumption that made implementation of the algorithm significantly easier and
%faster.  There are several different ways in which the algorithms presented in
%this thesis could be extended to handle color information.  One of the main
%reasons that color is not as important in face recognition as it is in some
%other vision applications is that the pixels in the image of a given person's
%face generally vary primarily in intensity.  Furthermore, the variation of
%skin tone from person to person usually varies less than the color variation
%resulting from the color of the illumination.  For these reasons, color may be
%especially useful for the improvement of occlusion handling, since occluding
%objects are likely to vary in color more than human faces do.  



