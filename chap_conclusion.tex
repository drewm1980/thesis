\chapter{Conclusion}
\label{chap:conclusion}

Humans have evolved an amazing capacity to recognize each-other with just a
glance at a person's face, from any angle, under almost any lighting condition,
and even if only a portion of the face is visible.  While we take it for
granted, the ability to quickly identify the people around us is critical to
our social interaction and often to our personal safety.  While we are often
able to recognize people we know by their gait, their voice, or even what they
are wearing, face recognition is the fastest and
most reliable way to identify people in our surroundings.  We are comforted by
seeing the faces of people we know and trust, and we are often ill at ease when
we are in the vicinity of someone who is concealing their identity with a mask.  

Face recognition is such a key component of human interaction that automatic
face recognition has the potential to be useful in almost any situation where a
human is interacting with a machine.  Indeed, automatic face recognition has
already become a large industry, even though, despite three decades of
research, the technology is still immature and very few applications have
actually been successful.

While the recognition system developed in this thesis is still far from
matching the robustness of the human recognition system, it demonstrates that
there is a largely ignored class of recognition applications for which face
recognition is tractable.  By gathering a sufficient set of images of each
subject under different illuminations and fixed pose, we are able to robustly
recognize subjects under lighting from any direction.  This parallels how
humans become familiar with the appearance of a face. Even if we only meet
someone briefly, as their head moves, so does the relative direction of the
light illuminating it, and thus we always see their face under varying
illumination.  While the necessity of gathering these gallery images increases
the cost of the system, this is outweighed by the robustness advantages.

Until quite recently, a recognition system of the type presented in this thesis
would have been extremely impractical.  In particular, until the advent of
Digital Micro-mirror Devices (a.k.a. DLP) by Texas Instruments, most projectors
had far lower contrast and lower brightness, and were thus much less suitable
for building a light stage like the described acquisition system.  Furthermore,
without the recent dramatic increases in computational bandwidth achieved by
modern parallel processors, the use of direct use of raw images as features was
prohibitively expensive; low dimensional feature extraction was a practical
necessity.  Finally, an improved understanding of the mathematical properties
of the $\ell_1$-norm, both as a robust error function, and also as a
sparsity-encouraging regularization term, enabled the formulation of recognition in
terms of convex optimization problems without sacrificing robustness to sparse
error (as with traditional $\ell_2$ norm error based methods).  The recognition
system presented here would have not been possible prior to the emergence of
these three technologies.

\section{Summary of Contributions}
While the primary value of this research is in the overall integrated approach taken by
the recognition system, there are several contributions that stand out, and could be
integrated into other recognition systems.  These contributions
are the acquisition system, the robust iterative alignment formulation,
improved $\ell_1$ minimization algorithms, and optimized parallel
implementations for both CPU and GPU architectures.  Each of these
contributions is briefly reviewed.

The acquisition system quickly captures images of the subject under a set of
illuminations that closely models the wide range of lighting distributions in
the real world.  It was shown empirically that 38 illuminations are sufficient
to accurately model a variety of real-world illumination conditions.  In
particular, experiments demonstrate that rear illuminations are important.  The
illumination generated by the system is distant, and with only two projectors
it covers well over a hemisphere of incident illumination directions; only a
cone of illumination directions from below horizontal is not covered.  With the
addition of a projector that bounces light off of the floor, the system could
even achieve true omnidirectional illumination coverage, though it remains to
be seen if this is worthwhile.  All of the hardware used is available off the
shelf, and the cost of the components is dropping rapidly.\footnote{The cost of
DLP projectors has roughly halved since the construction of the first
prototype.} The cost of the overall system is already comparable to the cost of
the illumination equipment already used in photography studios.  

The robust iterative alignment formulation successfully combines Lucas-Kanade
style iterative alignment with the $\ell_1$ norm as a robust error function,
and with an illumination model formed from the linear span of multiple gallery
images per subject.  It exhibits a region of attraction that is more than
sufficient to work with the output of the de-facto standard Viola-Jones face
detector.  In addition to boosting the performance of a SRC-based recognition
stage, iterative alignment is also shown to boost the performance of a
recognition stage based on local binary patterns.  Experiments on the Multi-PIE
dataset demonstrate that while this improves recognition rate, SRC remains
significantly better at rejecting impostors.  Isolating the cause of this and
designing a recognition stage with the strengths of both is an important topic
of future research.

In addition to the interior-point based $\ell_1$ solvers that were developed
for the first system prototype, solvers based on ALM were developed that
significantly reduce the amount of required computation, and are simpler 
to implement on parallel architectures.  In contrast to the matrix-matrix
operations and data re-ordering that are required by interior point solvers,
the inner loops of the ALM solvers are composed primarily of matrix-vector
multiplications and vector-vector operations.

Even with the improved solvers, optimized implementations running on powerful
parallel hardware are necessary to achieve recognition speeds that are
practical for access control systems (and many other applications by
extension).  While use of vendor-supplied parallel BLAS libraries is sufficient
to achieve efficient parallelization of the SRC based recognition stage, for
the alignment stage it is necessary to take advantage of the additional level
of parallelism in the algorithm that arises from performing alignment
individually for each gallery user.  On the CPU, this was achieved by using a
combination of single-threaded BLAS libraries, manual threading via the OpenMP
API, and auto-vectorization compiler features.  For alignment on the CPU, the
highest performing parallelization is achieved by solving one iterative
alignment problem per CPU core.  In contrast with the CPU, on the GPU, there
are no vendor-supplied libraries that operate at the granularity of a single
core (Streaming Multiprocessor).  The vendor supplied CUBLAS library is
designed to take advantage of the whole GPU to solve a single large problem.
It was determined experimentally that the alignment problems are much too small
to run efficiently on the GPU sequentially. It was therefore necessary to
develop an implementation of the alignment stage from scratch (i.e. no library
dependencies) that solves many alignment problems concurrently.  The optimized
implementations on both architectures are at least an order of magnitude faster
than their sub-optimally parallelized predecessors.  The CPU and GPU
architectures are competitive with each-other in speed.  On the CPU the problem
fits in L3 cache and speed is governed by the number of available cores.  On
the GPU the data must be streamed from DRAM, and speed is governed by memory
bandwidth.  Furthermore, new architectures are emerging that combine the best
aspects of CPUs (ample amounts of cache) and GPUs (massive concurrency).
Examples of this are AMD's Fusion Accelerated Processing Unit and Intel's
Knights Corner.  The parallelization lessons learned on the current generation
of hardware will almost certainly apply to the next generation of hardware as
well.


\section{Future Work}

%Schenker

%\section{Optimizing the Speed of the Recognition Algorithm}

%\begin{table}[h]
%\centering
%\begin{tabular}{|c|c|}
%\hline
%Operation & Execution Time (seconds)\\
%\hline
%Loading the training database & 74\\
%\hline
%Per-user alignment & 130\\
%\hline
%Resampling the training images & 18\\
%\hline
%Final recognition step &137\\
%\hline
%\end{tabular}\vspace{2mm}
%\caption{Execution timing for a database of 100 training users with 30 xga grayscale training images per user} \label{tab:breakdown}
%\end{table}

%\subsection{Loading the training database}  

%One strategy for reducing the latency of the image loading is to use a
%technology called memory mapping.  This creates a mapping between the data in a
%file in the system (most of which will be resident on the hard drive) and the
%address space of the program.  The operating system's virtual memory system
%then handles the loading of data from disk into RAM as needed when the program
%access the corresponding addresses in its memory space.  This technique has the
%potential to kill several birds with one stone:
%\begin{itemize}
%\item Since there is one virtual memory system for all processes in the os,
%portions of the database that are used by multiple processes can share the data
%that has been loaded into RAM.  This can significantly reduce the memory
%footprint when multiple instances of the recognition system are running
%simultaneously.
%\item It is much easier to load just the portion of each image file that is
%needed.  Resampling the training images will only require a subset of the
%pixels from the high resolution image.  On x86 systems the memory page size is
%2KB.  For a grayscale xga image laid out linearly one disk this corresponds to
%two rows of the image.  In this case only the rows that overlap the user's face
%will get loaded from disk into memory.  This can be improved further by laying
%out the data so that each memory corresponds to a tile of about 45 x 45 pixels.
%Then only the tiles that overlap the users' face will have to get loaded into
%memory.
%\item There is a mechanism to hint to the operating system what data will be
%needed in advance so that is can be pre-loaded.  
%\end{itemize} 
%Another strategy for reducing the latency of the image loading is to try to
%predict which images will need to be loaded in advance of when they are used.
%In particular, we have some information about which users will likely make the
%cut for the recognition step while per-user alignment is still being performed.
%Any training user that is not in the top $S$ (currently set to 10) users
%aligned so far certainly need not be loaded into memory.  

%\subsection{Resampling the training images}  
%One strategy for reducing the amount of time spent resampling(warping) the
%training images is to make use of the fact that we are resampling $N$ (= 38
%training images currently) simultaneously with the same transformation. 
%In other words, the training images can be treated as a single 38 color image.
%This this has the potential to speed up the resampling stage for two reasons:
%first, it avoids re-computation of the mapping of pixel locations, and second,
%the memory accesses have improved cache locality.
%Unfortunately, optimized third party libraries such as Intel's Integrated Performance
%Primitives rarely provide optimized image
%transformations for images of more than four colors.

%\section{Optimizing the training illuminations} 
%To date, I have performed two experiments have been performed to guide the
%choice of training illuminations, the results of which were shown in Chapter
%\ref{chap:pipeline}.  One used rectangular partitions of the domain of the
%illumination space with different granularities.  This gave a rough idea of the
%number of training illuminations that would be necessary.  The second
%experiment used a partitions of the illumination domain that were rectangular
%in a polar coordinate system.  This gave a rough idea of the range of angles
%the training images should cover.  Our large training database was acquired by
%combining these two pieces of information.  While a study of the choice of
%training illuminations at this granularity is already unprecedented for face
%recognition, \footnote{ Several studies, including \cite{Basri2003-PAMI,
%LeeK2005-PAMI} have used images rendered from a 3D face model to optimize their
%choice of illumination.  Similar illumination studies have also been performed
%on other objects for graphics applications.} only a very small subset of the
%possible illuminations was explored compared to what the projectors are capable
%of.  For instance, how many of the frontal images are necessary?  Should some
%directions receive a higher density of illuminations than others? 

%\subsection{Optimize illuminations within the set of training images we already have}
%One option we have for optimizing our training illuminations is to use the
%training and testing databases we have already gathered, and to search for a
%lower dimensional subspace of illuminations that still has a good recognition
%rate.  The two main advantages of this strategy are that the recognition rate
%can be computed directly and used as a measure of the quality of the training
%images, and that the sizable database we have already gathered can be used.
%The main disadvantage is that we will most likely have to trade some accuracy
%for speed, since we can only restrict the number of measurements the algorithm
%uses. \footnote{It could be possible for a subset of the illuminations is more
%discriminating than the full set; however this would be a very surprising
%result indeed!}

%\subsection{Optimize illuminations with the illumination system in the loop}
%A second option is to perform an optimization of the training image space with
%the image acquisition system in the loop.  The main advantage is that we can
%include any training images we want to in the optimizaztion.  The main
%disadvantage is that we cannot compute a recognition rate and instead have to
%resort to using representation error as a measure of the quality of the
%training images.  Why can't we just capture and store a the images generated by
%a complete basis for the space of illuminations, and then optimize over
%weighted combinations of them?  If a single projector pixel is illuminated at a
%time, it would take over 4 TB of data to store the remaining images, and take
%28 hours to capture; this wouldn't be convenient, but it could be managed.
%Unfortunately, there is a problem with this idea:  in every image you take, the
%actual signal would fall below the noise floor of the camera.  For this reason
%it is critical to capture images with a significant portion of the projector
%pixels illuminated at a time.  Keeping the real world in the loop solves this
%problem without resorting to arbitrarily enforcing a minimum block size.  Due
%to the extremely long time that the subject will have to remain motionless
%while the optimization runs, a movie-grade dummy head would be a good candidate
%for the training subject.  

%This still leaves several interesting questions.  Even with an automated
%search, we are going to have to make some assumptions to reduce the search
%space.  Should we allow for pixels to be partially turned on, or should they
%be binary?  Should we require illuminations to have pixels that are all
%adjacent?  What metric should we use to measure the quality of the training
%images?  Ideally we'd be using a large database of real subjects, but that is
%not an option if we want to search over the full set of illuminations the
%projectors can generate.  How many training images should we allow?  How do we
%quantify the tradeoff between speed and accuracy?

%There are few other things that make this experiment appealing.  Since the
%dummy head is inanimate, we will completely eliminate the influence of pose
%variation on the experiment, and all of our training images will already be
%perfectly aligned; the cameras can be manually positioned such that the
%frontal and rear illuminations match up perfectly. \footnote{Since we want to
%capture training illuminations from both sides, the dummy head can be mounted
%on a stepper-motor driven pivot for the purpose of rotating the dummy head by
%180 degrees.  }

%\subsection{Leverage color information to improve occlusion robustness}  Color
%information is unused for the current system.  This was a simplifying
%assumption that made implementation of the algorithm significantly easier and
%faster.  There are several different ways in which the algorithms presented in
%this thesis could be extended to handle color information.  One of the main
%reasons that color is not as important in face recognition as it is in some
%other vision applications is that the pixels in the image of a given person's
%face generally vary primarily in intensity.  Furthermore, the variation of
%skin tone from person to person usually varies less than the color variation
%resulting from the color of the illumination.  For these reasons, color may be
%especially useful for the improvement of occlusion handling, since occluding
%objects are likely to vary in color more than human faces do.  



%\section{Improving the acquisition system hardware}
%The current design has proven to be very satisfactory in a research environment where the subjects are very cooperative and make an effort to hold very still while the images are being taken.  Unfortunately, not all users of this technology will be so careful.  The best way to improve the quality of the training images is to increase the speed at which they are acquired.  The primary motive for this is to reduce the amount of movement of the user's head between consecutive training images.  There are several possible ways in which this goal can be pursued:
%%{\bf Refinements to the current design}
%\begin{itemize}
%\item {\em Increase the image acquisition rate by upgrading the cameras.}  Some newer IEEE1394 cameras contain features that could facilitate this.  One is the ability to trigger off of an external signal while still interleaving image exposure with transfer of the previous frame over the bus.  With our current cameras transfer and integration is serialized.  
%\item {\em Implementing some type automatic exposure control}  Some illuminations cause more light to fall on the subject's face than others.  Therefore, the optimal exposure time depends on which illumination is being displayed.  Automatic compensation for this could reduce the average exposure time without hurting the SNR.
%\item {\em Reduce synchronization delays by modifying the projectors}  There is a clock signal that drives the switching of the Digital Micromirror Device in each projector.  Synchronizing the camera exposure with a whole number of projector frames would greatly increase acquisition time; currently we have to be very conservative with the amount of time a given illumination is displayed to prevent problems synchronizing with the camera.
%\item {\em Reduce exposure time by modifying the projectors}  The only way to decrease the exposure time without hurting signal to noise ratio is to increase the intensity of the lighting.  Most Digital Light Processor (DLP) based projectors achieve color by spinning a wheel with either a colored mirror or a transmissive color filters in front of the light source.  Since color is not needed for  the training image acquisition, it would be desirable to remove the color wheel, effectively turning the projector into a black and white projector with a higher maximum intensity.
%\end{itemize}
%Not all of the above strategies may turn out to be feasible; in particular, the last two ideas would require some degree of reverse engineering of proprietary hardware inside the projector.  While the modifications are conceptually very simple, there may be unforeseen difficulties depending on the design of the projector.  In the worst case, the manufacturer may have deliberately implemented features to prevent tampering.

% TODO Mention scheneker illumination patterns.


%In other words, the algorithm 
%extends Lucas-Kanade iterative alignment \cite{LucasB1981} to use the
%$\ell_1$-norm as a robust error function, while simultaneously estimating
%the coefficients of the linear illumination model $A_i\x$.  

%We argue that in addition to vendor supplied libraries
%that attempt to leverage both levels of concurrency of the GPU, there is a need
%for standard libraries that operate at the SM level of concurrency.


